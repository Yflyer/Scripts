#http://benjjneb.github.io/dada2/tutorial.html help page of dada2 esv.
library(dada2)

# 1 # install dada2
source("https://bioconductor.org/biocLite.R")
biocLite("dada2")

# 2 # load dada2 and data
library(dada2); packageVersion("dada2")

path <-"X:/Jiajie Feng/Reproducibility/NCBI_reprod"  #this time run on 240 server, X:

prefix="ons_16s"
memory.limit(size=100*1024)
#list.files(path)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_F_repaired.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R_repaired.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq 
#this line defines the string before the separator as sample names. The example of the help page (the line below) has no "_" in the sample names, so they separate using "_". We have "_" in the sample names, so we use "_F_" in the fnFs names.
sample.names <- sapply(strsplit(basename(fnFs), "_F_"), `[`, 1)

# 3 # check quality
#this step only exhibits the first 2 files' qualities, don't know why. Can be skipped.
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])

# 4 # filter and trim
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

#maxN could be set as 1 or 2. After all, the Ns may be fixed when flash-comb
#truncLen strangething: set to 210 generates less seq than 200, but when set to 0 or completely delete "truncLen", it gets the least seqs.
#setting as c(100,100) or 100 would generate 0 paired-reads in the mergePairs step. Don't know why. This truncLen is really a bullshit parameter. Just set as 200. I have checked the f and r reads, they are both among lengths between 224 and 245. 
#formal. Here! Run this!(jnote06292019, also for cong 16s esv
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=200,
                          maxN=2, maxEE=c(2,2), truncQ=2, rm.phix=F,
                          compress=TRUE, multithread=FALSE)
# On Windows set multithread=FALSE
# On Linux set multithread=TRUE
out

# 5 # Learn the error rates
errF <- learnErrors(filtFs, multithread=FALSE)
errR <- learnErrors(filtRs, multithread=FALSE)

plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)

# 6 # Dereplication
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

# 7 # Sample inference # core alrightm in DADA
#jiajienote should be the error correction step in dada.
dadaFs <- dada(derepFs, err=errF, multithread=FALSE)
dadaRs <- dada(derepRs, err=errR, multithread=FALSE)

dadaFs[[1]]
dadaRs[[1]]

# 8 # Merge paired reads

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs,
                      minOverlap = 50, maxMismatch = 5, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

# 9 # Construct sequence table

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths  #most are 253 bp for 16s
table(nchar(getSequences(seqtab)))

# 10 # Remove chimeras

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
write.csv(data.frame(Sequence=colnames(seqtab.nochim),t(seqtab.nochim)),file=paste0(path,"/",prefix,".SeqTable.csv"),row.names=FALSE)

# 11 # Track reads through the pipeline
#jiajienote: this step shows how many ori seqs lost at each step
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
write.csv(data.frame(SampleID=rownames(track),track),file=paste0(path,"/",prefix,".TrackReads.csv"),row.names=FALSE)

# 12 # Assign taxonomy
#from https://www.dropbox.com/s/7wl3stvms693eww/IEGWS2018.Div.examples.r?dl=0 clas.conf=0.5 #if the fragment length is nearly or less than 250 bp
# download latest databases from https://benjjneb.github.io/dada2/training.html
# jnote: use rdp database http://rdp.cme.msu.edu/classifier/classifier.jsp 16S rRNA training set 16, for ITS should be...not sure (according to http://benjjneb.github.io/dada2/ITS_workflow.html, maybe the UNITE one works?). Just save the representative seqs from the .SeqTable.csv file. To generate a fasta file from the csv, find Jiajie Feng/AK sequencing/Phylogeny issue/Galaxy352p.xlsx. If still unclear, sad. Ask Jiajie.
ref.file="C:/Users/ndl81/Dropbox/Training/IEG.Workshop.2018/2-Diversity/DADA2.test/ReferenceDatabase/RDP16S/rdp_train_set_16.fa.gz"
taxa <- assignTaxonomy(seqtab.nochim, ref.file, multithread=FALSE)
colnames(taxa)


ref.sp="C:/Users/ndl81/Dropbox/Training/IEG.Workshop.2018/2-Diversity/DADA2.test/ReferenceDatabase/RDP16S/rdp_species_assignment_16.fa.gz"
taxa <- addSpecies(taxa, ref.sp)
colnames(taxa)
write.csv(data.frame(Sequence=rownames(taxa),taxa),file=paste0(path,"/",prefix,".Taxonomy.RDP.csv"),row.names=FALSE)

# use silva database
ref.file="C:/Users/ndl81/Dropbox/Training/IEG.Workshop.2018/2-Diversity/DADA2.test/ReferenceDatabase/Silva16S/silva_nr_v132_train_set.fa.gz"
ref.file="Y:/Jiajie Feng/OK NEW SIP/ESV table/silva_nr_v132_train_set.fa.gz"
taxa <- assignTaxonomy(seqtab.nochim, ref.file, multithread=FALSE)
colnames(taxa)

ref.sp="C:/Users/ndl81/Dropbox/Training/IEG.Workshop.2018/2-Diversity/DADA2.test/ReferenceDatabase/Silva16S/silva_species_assignment_v132.fa.gz"
taxa <- addSpecies(taxa, ref.sp)
colnames(taxa)
write.csv(data.frame(Sequence=rownames(taxa),taxa),file=paste0(path,"/",prefix,".Taxonomy.Silva.csv"),row.names=FALSE)
